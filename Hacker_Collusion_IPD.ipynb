{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0cc5472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part of code is the Q learning brain, which is a brain of the agent.\n",
    "All decisions are made in here.\n",
    "\n",
    "View more ``on my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37676c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningTable:\n",
    "    def __init__(self,learning_rate=0.05, reward_decay=0.9, e_greedy=0.3):\n",
    "        self.actions = [\"c\",\"d\"] \n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation,g):\n",
    "        self.check_state_exist(observation,g)\n",
    "        # action selection\n",
    "        if np.random.uniform(0,1) > self.epsilon:\n",
    "            # choose best action\n",
    "            state_action = self.q_table.loc[observation, :]\n",
    "            # some actions may have the same value, randomly choose on in these actions\n",
    "            action = np.random.choice(state_action[state_action == np.max(state_action)].index)\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "        return action\n",
    "    \n",
    "    def learn(self,current_state,action,g,next_state,reward):\n",
    "        self.check_state_exist(next_state,g)\n",
    "        q_predict = self.q_table.loc[current_state, action]\n",
    "        q_target = q_predict + self.lr*(reward + self.gamma*np.max(self.q_table.loc[next_state,:]) - q_predict )\n",
    "#         if action == \"c\":\n",
    "#             q_target = q_predict + self.lr*(1- self.epsilon/2)*((1- self.epsilon/2)*2*g + self.epsilon/2 * g + (self.gamma - 1)*q_predict)\n",
    "#         elif action == \"d\":\n",
    "#             q_target = q_predict + self.lr* self.epsilon/2 *((1- self.epsilon/2)*(2+g) + self.epsilon * g + (self.gamma*self.q_table.loc[next_state, \"c\"] - self.q_table.loc[next_state, action]))\n",
    "\n",
    "        \n",
    "        \n",
    "        self.q_table.loc[current_state,action] = q_target\n",
    "\n",
    "    def check_state_exist(self, state,g):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series(\n",
    "                    [2*g/(1-self.gamma)]*len(self.actions),\n",
    "                    index=self.q_table.columns,\n",
    "                    name=state,\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5460019a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 0 ends in state cc for player 1 and player 2\n",
      "Game 1 ends in state cc for player 1 and player 2\n",
      "Game 2 ends in state cd for player 1 and player 2\n",
      "Game 3 ends in state dd for player 1 and player 2\n",
      "Game 4 ends in state cc for player 1 and player 2\n",
      "Game 5 ends in state dc for player 1 and player 2\n",
      "Game 6 ends in state dd for player 1 and player 2\n",
      "Game 7 ends in state dc for player 1 and player 2\n",
      "Game 8 ends in state cd for player 1 and player 2\n",
      "Game 9 ends in state cd for player 1 and player 2\n",
      "{'dc': 182046, 'cd': 190651, 'dd': 250810, 'cc': 376493}\n",
      "game over\n"
     ]
    }
   ],
   "source": [
    "#create the environment for IPD and run the experiment and plot the graphs. \n",
    "import pdb\n",
    "g = 1.8\n",
    "state_data = {}\n",
    "\n",
    "def collect(observation):\n",
    "    if observation not in state_data:\n",
    "        state_data[observation] = 1\n",
    "    else: \n",
    "        state_data[observation]+=1\n",
    "\n",
    "def reward_joint(action1,action2,g):\n",
    "    payoff_matrix = {\n",
    "                     \"cc\": [2*g,2*g],\n",
    "                     \"cd\":[g,2+g],\n",
    "                     \"dc\": [2+g,g],\n",
    "                     \"dd\":[2,2]\n",
    "                    }\n",
    "    return payoff_matrix[action1 + action2]\n",
    "    \n",
    "\n",
    "for episode in range(10):\n",
    "    # initial observation\n",
    "\n",
    "    agent1 = QLearningTable()\n",
    "    agent2 = QLearningTable()\n",
    "    observation = \"cc\"\n",
    "#     pdb.set_trace()\n",
    "    for iterations in range(100000):\n",
    "\n",
    "        # RL choose action based on observation\n",
    "        action1 = agent1.choose_action(str(observation),g)\n",
    "        action2 = agent2.choose_action(str(observation),g)\n",
    "\n",
    "        # RL take action and get next observation and reward\n",
    "        observation_ = action1 + action2\n",
    "        reward = reward_joint(action1,action2,g)\n",
    "#         pdb.set_trace()\n",
    "        # RL learn from this transition\n",
    "        agent1.learn(str(observation), action1,g,str(observation_),reward[0])\n",
    "        agent2.learn(str(observation), action2,g,str(observation_),reward[1])\n",
    "\n",
    "        # swap observation\n",
    "        observation = observation_\n",
    "        collect(observation)\n",
    "\n",
    "        # break while loop when end of this episode\n",
    "#             if done:\n",
    "#                 break\n",
    "    print(f\"Game {episode} ends in state {observation} for player 1 and player 2\")\n",
    "\n",
    "print(state_data)\n",
    "# end of game\n",
    "print('game over')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb4e230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
